{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cascade Correlation",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMw+0HwQ6owp9U9nibL6mRb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackjameswillis/BSc-Final-Year-Project/blob/main/Cascade_Correlation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpS2ywuQ08o7"
      },
      "source": [
        "#Important Notes\r\n",
        "\r\n",
        "This notebook is incomplete. Known problems:\r\n",
        "\r\n",
        "*  Custom loss function does not recieve batches, just single instances of label/logit pairs(?).\r\n",
        "*   Loss function is incorrect.\r\n",
        "*   Hidden layer optimiser implements gradient descent when it should be ascent.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__GxjpXuYKc1"
      },
      "source": [
        "We begin by importing libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHowATxorKB"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92bhXTFOYQ8Y"
      },
      "source": [
        "Also define a seed to keep track of the order of data after shuffling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L2OF3xroR7c"
      },
      "source": [
        "# Seed required for tracking and altering inputs from input function\r\n",
        "# using values generated per layer. Seed value is candidate no.\r\n",
        "seed = 201338 "
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-WL4bY8YZzS"
      },
      "source": [
        "We download the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFdUFtQkirz1"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNS2WQIBYgid"
      },
      "source": [
        "The Cascade Correlation algorithm requires the manipulation of input data. Therefore, our data generation function is quite important. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BTT8pBeintN"
      },
      "source": [
        "# The key for the input features. We have one set of inputs,\r\n",
        "# these being the flattened images, so we provide one key.\r\n",
        "feat_key = \"x\"\r\n",
        "\r\n",
        "# We use this generator function to produce the dataset later.\r\n",
        "def generator(X, Y):\r\n",
        "\r\n",
        "  def _gen():\r\n",
        "\r\n",
        "    for x, y in zip(X, Y):\r\n",
        "\r\n",
        "      yield tf.reshape(x, [len(x)]), y\r\n",
        "\r\n",
        "  return _gen\r\n",
        "\r\n",
        "# We map this function onto elements of the dataset \r\n",
        "# to make it compatible with the expected format of \r\n",
        "# input data.\r\n",
        "def featurise(x, y):\r\n",
        "  \r\n",
        "  features = {feat_key: x}\r\n",
        "\r\n",
        "  return features, y\r\n",
        "\r\n",
        "# This input function returns another function that returns a tensorflow dataset\r\n",
        "# which is used to feed data into models. We have parameters on this function\r\n",
        "# generator that allow us to indicate and manipulate features and labels.\r\n",
        "def input_fn(partition, batch_size, repeat=False, alter_x=False, x_alterations=False, alter_y=False, y_alterations=False):\r\n",
        "\r\n",
        "  if partition == 'train':\r\n",
        "\r\n",
        "    x, y = (x_train, y_train)\r\n",
        "\r\n",
        "  elif partition == 'debug':\r\n",
        "\r\n",
        "    x, y = (x_train[:1], y_train[:1])\r\n",
        "    \r\n",
        "  else:\r\n",
        "\r\n",
        "    x, y = (x_test, y_test)\r\n",
        "\r\n",
        "  # We take whichever of training and testing data\r\n",
        "  # and first randomize the their order.\r\n",
        "  rng = np.random.default_rng(seed=seed)\r\n",
        "\r\n",
        "  rng.shuffle(x)\r\n",
        "\r\n",
        "  rng = np.random.default_rng(seed=seed)\r\n",
        "\r\n",
        "  rng.shuffle(y)\r\n",
        "\r\n",
        "  # In this section we check to see if data needs to be altered. In the case \r\n",
        "  # where we want to alter the labels, we simply assign the given alterations\r\n",
        "  # to be the labels. This facilitates the alternating training between\r\n",
        "  # regression in the hidden layers and classification in the output layer.\r\n",
        "  if alter_y: \r\n",
        "\r\n",
        "    y = np.asarray(y_alterations)\r\n",
        "\r\n",
        "\r\n",
        "  # In the case where we want to alter the features, we append given alterations\r\n",
        "  # onto each pre-existing feature set. This facilitates the increasing input\r\n",
        "  # size for each neuron added to the model. Aside from alterations, this code\r\n",
        "  # also squishes and flattens the features.\r\n",
        "  _x = []\r\n",
        "\r\n",
        "  # If we want to alter features and have alterations:  \r\n",
        "  if alter_x and len(x_alterations) != 0:\r\n",
        "\r\n",
        "    for i in range(len(x)):\r\n",
        "\r\n",
        "      _x.append(x[i]/255)\r\n",
        "\r\n",
        "      _x[i] = np.concatenate((_x[i].flatten(),x_alterations[i]))\r\n",
        "    \r\n",
        "    x = np.asarray(_x)\r\n",
        "      \r\n",
        "  else:\r\n",
        "\r\n",
        "    for i in range(len(x)):\r\n",
        "\r\n",
        "      _x.append(x[i]/255)\r\n",
        "\r\n",
        "      _x[i] = _x[i].flatten()\r\n",
        "      \r\n",
        "    x = np.asarray(_x)\r\n",
        "\r\n",
        "  # This is the function returned, which generates the \r\n",
        "  # dataset from the preprocessed data.\r\n",
        "  def _input_fn():\r\n",
        "    \r\n",
        "    # Make sure that our generator is producing a dateset of the correct data \r\n",
        "    # type depending on alterations made to the labels\r\n",
        "    y_type = tf.float32 if alter_y else tf.int32\r\n",
        "\r\n",
        "    # Use our generator function to produce the dataset\r\n",
        "    # with the appropriate data sizes and data types.\r\n",
        "    dataset = tf.data.Dataset.from_generator(generator(x, y), (tf.float32, y_type), ((len(x[0])), ()))\r\n",
        "\r\n",
        "    # If we are training, we want training to continue\r\n",
        "    # after we reach the final batch of the dataset. We\r\n",
        "    # achieve this by 'repeating' the dataset.\r\n",
        "    if repeat: dataset = dataset.repeat()\r\n",
        "\r\n",
        "    # Apply featre function to data and batch according\r\n",
        "    # to batch_size parameter.\r\n",
        "    dataset = dataset.map(featurise).batch(batch_size)\r\n",
        "\r\n",
        "    # Return the dataset ready to train on.\r\n",
        "    return dataset\r\n",
        "\r\n",
        "  return _input_fn"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqU8w3ewLoG4"
      },
      "source": [
        "# This functions is useful for calculating error in classification by converting\r\n",
        "# model output to binary.\r\n",
        "def binary_representation(Y):\r\n",
        "\r\n",
        "  return np.asarray([[1 if i==y else 0 for i in range(10)] for y in Y])"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnyWWkL9a3nk"
      },
      "source": [
        "The Cascade Correlation algorithm comprises of a number of key steps:\r\n",
        "* Train a model on top that is connected to each input and each layer.\r\n",
        "* Get the errors of the top model.\r\n",
        "* Train a new hidden unit to maximise the correlation between its output and\r\n",
        "the error of the top model with inputs of the original input features and all previous hidden units.\r\n",
        "* Freeze the new hidden unit's weights.\r\n",
        "* Repeat.\r\n",
        "\r\n",
        "Our code needs to facilitate this. Our input function generator allows us to provide the different types of unit in our system with the data they need. We now need to construct the architecture and feed the data into it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_99UpTBjehMq"
      },
      "source": [
        "class CassCor:\r\n",
        "\r\n",
        "  # Each hidden unit effectively corresponds to a new hidden layer.\r\n",
        "  layers = []\r\n",
        "\r\n",
        "  # The top layer is treated independently of the rest since it has a unique\r\n",
        "  # function.\r\n",
        "  top_layer = None\r\n",
        "\r\n",
        "  top_head = None\r\n",
        "\r\n",
        "  top_feature_columns = None\r\n",
        "\r\n",
        "  input_dimension = None\r\n",
        "\r\n",
        "  def __init__(self, feature_columns, top_head, input_dimension):\r\n",
        "\r\n",
        "    self.top_head = top_head\r\n",
        "\r\n",
        "    self.top_feature_columns = feature_columns\r\n",
        "\r\n",
        "    self.input_dimension = input_dimension\r\n",
        "\r\n",
        "  # The feedforward of this architecture requires us to produce a new dataset\r\n",
        "  # for each layer.\r\n",
        "  def feedforward_layers(self, partition, batch_size):\r\n",
        "\r\n",
        "    additions = []\r\n",
        "\r\n",
        "    y = None\r\n",
        "\r\n",
        "    # For each hidden unit.\r\n",
        "    for l in self.layers:\r\n",
        "\r\n",
        "      # Generate the input function.\r\n",
        "      current_input_fn = input_fn(\r\n",
        "          \r\n",
        "          partition=partition,\r\n",
        "\r\n",
        "          batch_size=batch_size,\r\n",
        "\r\n",
        "          alter_x=True,\r\n",
        "\r\n",
        "          x_alterations=additions)\r\n",
        "\r\n",
        "      # Get its predictions\r\n",
        "      logits = list(l.predict(input_fn=current_input_fn))\r\n",
        "\r\n",
        "      # If there are already hidden unit logits, add the new logits to the list.\r\n",
        "      if additions:\r\n",
        "        \r\n",
        "        additions = [additions[i] + list(logits[i]['predictions']) for i in range(len(logits))]\r\n",
        "\r\n",
        "      # If there are no hidden unit logits yet, create the appropriate structure\r\n",
        "      # for the new logits.\r\n",
        "      else:\r\n",
        "\r\n",
        "        additions = [list(logits[i]['predictions']) for i in range(len(logits))]\r\n",
        "\r\n",
        "    return y, additions\r\n",
        "  \r\n",
        "  # The residual errors are an important part of the loss function for the\r\n",
        "  # hidden units of this system.\r\n",
        "  def residual_errors(self, partition, batch_size):\r\n",
        "\r\n",
        "    # Generate the required feature additions for the top layer.\r\n",
        "    _, additions = self.feedforward_layers(\r\n",
        "        \r\n",
        "        partition=partition,\r\n",
        "        \r\n",
        "        batch_size=batch_size)\r\n",
        "    \r\n",
        "    # Generate an input function used to get top layer output.\r\n",
        "    predict_input_fn = input_fn(\r\n",
        "        \r\n",
        "        partition=partition,\r\n",
        "\r\n",
        "        batch_size=batch_size,\r\n",
        "\r\n",
        "        alter_x=True,\r\n",
        "\r\n",
        "        x_alterations=additions)\r\n",
        "    \r\n",
        "    # Generate predictions of the top layer on the generated input function.\r\n",
        "    predictions = self.top_layer.predict(\r\n",
        "        \r\n",
        "        input_fn=predict_input_fn)\r\n",
        "    \r\n",
        "    # Get the classification for each input.\r\n",
        "    predictions = [prediction['class_ids'] for prediction in predictions]\r\n",
        "    \r\n",
        "    # Get the appropriate labels for calculating error.\r\n",
        "    if partition == 'train':\r\n",
        "\r\n",
        "      Y = y_train\r\n",
        "\r\n",
        "    else:\r\n",
        "\r\n",
        "      Y = y_test\r\n",
        "\r\n",
        "    # Shuffle the labels in the same way they were shuffled in the \r\n",
        "    # input function.\r\n",
        "    rng = np.random.default_rng(seed=seed)\r\n",
        "\r\n",
        "    rng.shuffle(Y)\r\n",
        "    \r\n",
        "    # Calculate the error.\r\n",
        "    error = np.asarray([1 if Y[i] == predictions[i][0] else 0 for i in range(len(predictions))])\r\n",
        "\r\n",
        "    # Use the error to calculate the mean error.\r\n",
        "    mean_error = sum(error)/len(error)\r\n",
        "\r\n",
        "    # Return the residual error.\r\n",
        "    return error - mean_error\r\n",
        "\r\n",
        "  # This function simply re-trains the top layer, which involves producing a \r\n",
        "  # new classifier. The top layer is then evaluated on a testing input function.\r\n",
        "  def train_test_top(self, train_input_fn, test_input_fn, steps):\r\n",
        "\r\n",
        "    self.top_feature_columns = [tf.feature_column.numeric_column(key=feat_key, shape=[self.input_dimension + len(self.layers)])]\r\n",
        "\r\n",
        "    self.top_layer = tf.estimator.DNNEstimator(\r\n",
        "        \r\n",
        "        head=self.top_head,\r\n",
        "\r\n",
        "        hidden_units=[],\r\n",
        "\r\n",
        "        feature_columns=self.top_feature_columns,\r\n",
        "\r\n",
        "        activation_fn=tf.nn.sigmoid,\r\n",
        "\r\n",
        "        optimizer='SGD')\r\n",
        "\r\n",
        "    self.top_layer.train(\r\n",
        "        \r\n",
        "        input_fn=train_input_fn,\r\n",
        "\r\n",
        "        max_steps=steps)\r\n",
        "\r\n",
        "    results = self.top_layer.evaluate(\r\n",
        "        \r\n",
        "        input_fn=test_input_fn,\r\n",
        "        \r\n",
        "        steps=None)\r\n",
        "    \r\n",
        "    print(\"Accuracy:\", results[\"accuracy\"])\r\n",
        "\r\n",
        "    print(\"Loss:\", results[\"average_loss\"])\r\n",
        "\r\n",
        "  # This function trains a new layer. It begins by defining a regressor to the\r\n",
        "  # required specification, trains on the provided training input function,\r\n",
        "  # and appends the regressor onto the layers of the model.\r\n",
        "  def train_layer(self, train_input_fn, loss_fn, steps):\r\n",
        "\r\n",
        "    new_layer_feature_columns = [tf.feature_column.numeric_column(key=feat_key, shape=[self.input_dimension + len(self.layers)])]\r\n",
        "\r\n",
        "    new_layer_head = tf.estimator.RegressionHead(label_dimension=1, loss_fn=loss_fn,)\r\n",
        "\r\n",
        "    new_layer = tf.estimator.DNNEstimator(\r\n",
        "        \r\n",
        "        head=new_layer_head,\r\n",
        "\r\n",
        "        hidden_units=[],\r\n",
        "\r\n",
        "        feature_columns=new_layer_feature_columns,\r\n",
        "\r\n",
        "        activation_fn=tf.nn.sigmoid,\r\n",
        "        \r\n",
        "        optimizer='SGD')\r\n",
        "    \r\n",
        "    new_layer.train(\r\n",
        "        \r\n",
        "        input_fn=train_input_fn,\r\n",
        "\r\n",
        "        max_steps=steps)\r\n",
        "    \r\n",
        "    self.layers.append(new_layer)\r\n",
        "  \r\n",
        "  # This function collects the other training functions together into one\r\n",
        "  # coherent procedure. Until a total number of training steps has been met,\r\n",
        "  # the top layer of the model is trained, followed by a new layer being trained.\r\n",
        "  def train_test(self, batch_size, total_steps, steps_per_unit):\r\n",
        "\r\n",
        "    # Since we train the top layer and a new layer we train two units each\r\n",
        "    # iterations, this 2*steps_per_unit.\r\n",
        "    for i in range(0,total_steps,2*steps_per_unit):\r\n",
        "      \r\n",
        "      # Generate additional features on training data.\r\n",
        "      _, train_additions = self.feedforward_layers(\r\n",
        "          \r\n",
        "          partition='train',\r\n",
        "          \r\n",
        "          batch_size=batch_size)\r\n",
        "      \r\n",
        "      # Generate a new training input function based on those features.\r\n",
        "      train_input_fn = input_fn(\r\n",
        "          \r\n",
        "          partition='train', \r\n",
        "          \r\n",
        "          batch_size=batch_size, \r\n",
        "          \r\n",
        "          repeat=True,\r\n",
        "\r\n",
        "          alter_x=True, \r\n",
        "          \r\n",
        "          x_alterations=train_additions)\r\n",
        "      \r\n",
        "      # Generate additional features on testing data.\r\n",
        "      _, test_additions = self.feedforward_layers(\r\n",
        "          \r\n",
        "          partition='test',\r\n",
        "          \r\n",
        "          batch_size=batch_size)\r\n",
        "\r\n",
        "      # Generate a new testing input function based on those features.\r\n",
        "      test_input_fn = input_fn(\r\n",
        "          \r\n",
        "          partition='test', \r\n",
        "          \r\n",
        "          batch_size=batch_size, \r\n",
        "\r\n",
        "          alter_x=True,\r\n",
        "          \r\n",
        "          x_alterations=test_additions)\r\n",
        "      \r\n",
        "      # Train and test the top layer.\r\n",
        "      self.train_test_top(\r\n",
        "          \r\n",
        "          train_input_fn=train_input_fn, \r\n",
        "          \r\n",
        "          test_input_fn=test_input_fn, \r\n",
        "          \r\n",
        "          steps=steps_per_unit)\r\n",
        "      \r\n",
        "      # Calculate the residual errors of training the new layer.\r\n",
        "      errors = self.residual_errors(\r\n",
        "          \r\n",
        "          partition='train',\r\n",
        "          \r\n",
        "          batch_size=batch_size)\r\n",
        "      \r\n",
        "      # Produce a new training input function for the new layer based on the\r\n",
        "      # previously generated training additions and the generated errors.\r\n",
        "      train_input_fn = input_fn(\r\n",
        "          \r\n",
        "          partition='train',\r\n",
        "\r\n",
        "          batch_size=batch_size,\r\n",
        "\r\n",
        "          repeat=True,\r\n",
        "\r\n",
        "          alter_x=True,\r\n",
        "\r\n",
        "          x_alterations=train_additions,\r\n",
        "\r\n",
        "          alter_y=True,\r\n",
        "\r\n",
        "          y_alterations=errors)\r\n",
        "      \r\n",
        "      # Generate and train the new layer.\r\n",
        "      self.train_layer(\r\n",
        "          \r\n",
        "          train_input_fn=train_input_fn,\r\n",
        "\r\n",
        "          loss_fn=loss_fn,\r\n",
        "\r\n",
        "          steps=steps_per_unit)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m23SDknHhU1r"
      },
      "source": [
        "The loss function will be defined as per the original paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_MNQ3mFldxT"
      },
      "source": [
        "def loss_fn(labels, logits):\r\n",
        "\r\n",
        "  loss = tf.math.abs(logits * labels)\r\n",
        "\r\n",
        "  return loss"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sdu-C_1Xoak"
      },
      "source": [
        "feature_columns = [tf.feature_column.numeric_column('x', shape=(28*28))]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W9VVZI6WsRV"
      },
      "source": [
        "top_head = tf.estimator.MultiClassHead(n_classes=10)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQidqMuwPVT1"
      },
      "source": [
        "model = CassCor(\r\n",
        "    \r\n",
        "    feature_columns=feature_columns,\r\n",
        "    \r\n",
        "    top_head=top_head,\r\n",
        "    \r\n",
        "    input_dimension=28*28)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP92Z6jqSvbL"
      },
      "source": [
        "model.train_test(\r\n",
        "    \r\n",
        "    batch_size=64,\r\n",
        "\r\n",
        "    total_steps=5000,\r\n",
        "\r\n",
        "    steps_per_unit=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZaS6_aFaEcF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}